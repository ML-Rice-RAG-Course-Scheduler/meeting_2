{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSVWDyS4K5Kn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "\n",
        "# Input \"layer\"\n",
        "class Layer_Input:\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        self.output = inputs\n",
        "\n",
        "\n",
        "# ReLU activation\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "    # Calculate predictions for outputs\n",
        "    def predictions(self, outputs):\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, training):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Calculate predictions for outputs\n",
        "    def predictions(self, outputs):\n",
        "        return np.argmax(outputs, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "    # Set/remember trainable layers\n",
        "    def remember_trainable_layers(self, trainable_layers):\n",
        "        self.trainable_layers = trainable_layers\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y, *, include_regularization=False):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Add accumulated sum of losses and sample count\n",
        "        self.accumulated_sum += np.sum(sample_losses)\n",
        "        self.accumulated_count += len(sample_losses)\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss\n",
        "\n",
        "    # Calculates accumulated loss\n",
        "    def calculate_accumulated(self, *, include_regularization=False):\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return data_loss\n",
        "\n",
        "    # Reset variables for accumulated loss\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values\n",
        "        correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "\n",
        "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "\n",
        "# Common accuracy class\n",
        "class Accuracy:\n",
        "\n",
        "    # Calculates an accuracy\n",
        "    # given predictions and ground truth values\n",
        "    def calculate(self, predictions, y):\n",
        "\n",
        "        # Get comparison results\n",
        "        comparisons = self.compare(predictions, y)\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = np.mean(comparisons)\n",
        "\n",
        "        # Add accumulated sum of matching values and sample count\n",
        "        self.accumulated_sum += np.sum(comparisons)\n",
        "        self.accumulated_count += len(comparisons)\n",
        "\n",
        "        # Return accuracy\n",
        "        return accuracy\n",
        "\n",
        "    # Calculates accumulated accuracy\n",
        "    def calculate_accumulated(self):\n",
        "\n",
        "        # Calculate an accuracy\n",
        "        accuracy = self.accumulated_sum / self.accumulated_count\n",
        "\n",
        "        # Return the data and regularization losses\n",
        "        return accuracy\n",
        "\n",
        "    # Reset variables for accumulated accuracy\n",
        "    def new_pass(self):\n",
        "        self.accumulated_sum = 0\n",
        "        self.accumulated_count = 0\n",
        "\n",
        "\n",
        "# Accuracy calculation for classification model\n",
        "class Accuracy_Categorical(Accuracy):\n",
        "\n",
        "    def __init__(self, *, binary=False):\n",
        "        # Binary mode?\n",
        "        self.binary = binary\n",
        "\n",
        "    # No initialization is needed\n",
        "    def init(self, y):\n",
        "        pass\n",
        "\n",
        "    # Compares predictions to the ground truth values\n",
        "    def compare(self, predictions, y):\n",
        "        if not self.binary and len(y.shape) == 2:\n",
        "            y = np.argmax(y, axis=1)\n",
        "        return predictions == y\n",
        "\n",
        "# Model class\n",
        "class Model:\n",
        "\n",
        "    def __init__(self):\n",
        "        # Create a list of network objects\n",
        "        self.layers = []\n",
        "        # Softmax classifier's output object\n",
        "        self.softmax_classifier_output = None\n",
        "\n",
        "    # Add objects to the model\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    # Set loss, optimizer and accuracy\n",
        "    def set(self, *, loss, optimizer, accuracy):\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.accuracy = accuracy\n",
        "\n",
        "    # Finalize the model\n",
        "    def finalize(self):\n",
        "\n",
        "        # Create and set the input layer\n",
        "        self.input_layer = Layer_Input()\n",
        "\n",
        "        # Count all the objects\n",
        "        layer_count = len(self.layers)\n",
        "\n",
        "        # Initialize a list containing trainable layers:\n",
        "        self.trainable_layers = []\n",
        "\n",
        "        # Iterate the objects\n",
        "        for i in range(layer_count):\n",
        "\n",
        "            # If it's the first layer,\n",
        "            # the previous layer object is the input layer\n",
        "            if i == 0:\n",
        "                self.layers[i].prev = self.input_layer\n",
        "                self.layers[i].next = self.layers[i+1]\n",
        "\n",
        "            # All layers except for the first and the last\n",
        "            elif i < layer_count - 1:\n",
        "                self.layers[i].prev = self.layers[i-1]\n",
        "                self.layers[i].next = self.layers[i+1]\n",
        "\n",
        "            # The last layer - the next object is the loss\n",
        "            # Also let's save aside the reference to the last object\n",
        "            # whose output is the model's output\n",
        "            else:\n",
        "                self.layers[i].prev = self.layers[i-1]\n",
        "                self.layers[i].next = self.loss\n",
        "                self.output_layer_activation = self.layers[i]\n",
        "\n",
        "            # If layer contains an attribute called \"weights\",\n",
        "            # it's a trainable layer -\n",
        "            # add it to the list of trainable layers\n",
        "            # We don't need to check for biases -\n",
        "            # checking for weights is enough\n",
        "            if hasattr(self.layers[i], 'weights'):\n",
        "                self.trainable_layers.append(self.layers[i])\n",
        "\n",
        "\n",
        "        # Update loss object with trainable layers\n",
        "        self.loss.remember_trainable_layers(\n",
        "            self.trainable_layers\n",
        "        )\n",
        "\n",
        "        # If output activation is Softmax and\n",
        "        # loss function is Categorical Cross-Entropy\n",
        "        # create an object of combined activation\n",
        "        # and loss function containing\n",
        "        # faster gradient calculation\n",
        "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
        "           isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
        "            # Create an object of combined activation\n",
        "            # and loss functions\n",
        "            self.softmax_classifier_output = \\\n",
        "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Train the model\n",
        "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
        "              print_every=1, validation_data=None):\n",
        "\n",
        "        # Initialize accuracy object\n",
        "        self.accuracy.init(y)\n",
        "\n",
        "        # Default value if batch size is not being set\n",
        "        train_steps = 1\n",
        "\n",
        "        # If there is validation data passed,\n",
        "        # set default number of steps for validation as well\n",
        "        if validation_data is not None:\n",
        "            validation_steps = 1\n",
        "\n",
        "            # For better readability\n",
        "            X_val, y_val = validation_data\n",
        "\n",
        "        # Calculate number of steps\n",
        "        if batch_size is not None:\n",
        "            train_steps = len(X) // batch_size\n",
        "            # Dividing rounds down. If there are some remaining\n",
        "            # data but not a full batch, this won't include it\n",
        "            # Add `1` to include this not full batch\n",
        "            if train_steps * batch_size < len(X):\n",
        "                train_steps += 1\n",
        "\n",
        "            if validation_data is not None:\n",
        "                validation_steps = len(X_val) // batch_size\n",
        "\n",
        "                # Dividing rounds down. If there are some remaining\n",
        "                # data but nor full batch, this won't include it\n",
        "                # Add `1` to include this not full batch\n",
        "                if validation_steps * batch_size < len(X_val):\n",
        "                    validation_steps += 1\n",
        "\n",
        "        # Main training loop\n",
        "        for epoch in range(1, epochs+1):\n",
        "\n",
        "            # Print epoch number\n",
        "            print(f'epoch: {epoch}')\n",
        "\n",
        "            # Reset accumulated values in loss and accuracy objects\n",
        "            self.loss.new_pass()\n",
        "            self.accuracy.new_pass()\n",
        "\n",
        "            # Iterate over steps\n",
        "            for step in range(train_steps):\n",
        "\n",
        "                # If batch size is not set -\n",
        "                # train using one step and full dataset\n",
        "                if batch_size is None:\n",
        "                    batch_X = X\n",
        "                    batch_y = y\n",
        "\n",
        "                # Otherwise slice a batch\n",
        "                else:\n",
        "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
        "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "                # Perform the forward pass\n",
        "                output = self.forward(batch_X, training=True)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = self.loss.calculate(output, batch_y)\n",
        "\n",
        "                # Get predictions and calculate an accuracy\n",
        "                predictions = self.output_layer_activation.predictions(\n",
        "                                  output)\n",
        "                accuracy = self.accuracy.calculate(predictions,\n",
        "                                                   batch_y)\n",
        "\n",
        "                # Perform backward pass\n",
        "                self.backward(output, batch_y)\n",
        "\n",
        "\n",
        "                # Optimize (update parameters)\n",
        "                self.optimizer.pre_update_params()\n",
        "                for layer in self.trainable_layers:\n",
        "                    self.optimizer.update_params(layer)\n",
        "                self.optimizer.post_update_params()\n",
        "\n",
        "                # Print a summary\n",
        "                if not step % print_every or step == train_steps - 1:\n",
        "                    print(f'step: {step}, ' +\n",
        "                          f'acc: {accuracy:.3f}, ' +\n",
        "                          f'loss: {loss:.3f}, ' +\n",
        "                          f'lr: {self.optimizer.current_learning_rate}')\n",
        "\n",
        "            # Get and print epoch loss and accuracy\n",
        "            epoch_loss = self.loss.calculate_accumulated()\n",
        "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "            print(f'training, ' +\n",
        "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
        "                  f'loss: {epoch_loss:.3f}, ' +\n",
        "                  f'lr: {self.optimizer.current_learning_rate}')\n",
        "\n",
        "            # If there is the validation data\n",
        "            if validation_data is not None:\n",
        "\n",
        "                # Reset accumulated values in loss\n",
        "                # and accuracy objects\n",
        "                self.loss.new_pass()\n",
        "                self.accuracy.new_pass()\n",
        "\n",
        "                # Iterate over steps\n",
        "                for step in range(validation_steps):\n",
        "\n",
        "                    # If batch size is not set -\n",
        "                    # train using one step and full dataset\n",
        "                    if batch_size is None:\n",
        "                        batch_X = X_val\n",
        "                        batch_y = y_val\n",
        "\n",
        "\n",
        "                    # Otherwise slice a batch\n",
        "                    else:\n",
        "                        batch_X = X_val[\n",
        "                            step*batch_size:(step+1)*batch_size\n",
        "                        ]\n",
        "                        batch_y = y_val[\n",
        "                            step*batch_size:(step+1)*batch_size\n",
        "                        ]\n",
        "\n",
        "                    # Perform the forward pass\n",
        "                    output = self.forward(batch_X, training=False)\n",
        "\n",
        "                    # Calculate the loss\n",
        "                    self.loss.calculate(output, batch_y)\n",
        "\n",
        "                    # Get predictions and calculate an accuracy\n",
        "                    predictions = self.output_layer_activation.predictions(\n",
        "                                      output)\n",
        "                    self.accuracy.calculate(predictions, batch_y)\n",
        "\n",
        "                # Get and print validation loss and accuracy\n",
        "                validation_loss = self.loss.calculate_accumulated()\n",
        "                validation_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "                # Print a summary\n",
        "                print(f'validation, ' +\n",
        "                      f'acc: {validation_accuracy:.3f}, ' +\n",
        "                      f'loss: {validation_loss:.3f}')\n",
        "\n",
        "    # Performs forward pass\n",
        "    def forward(self, X, training):\n",
        "\n",
        "        # Call forward method on the input layer\n",
        "        # this will set the output property that\n",
        "        # the first layer in \"prev\" object is expecting\n",
        "        self.input_layer.forward(X, training)\n",
        "\n",
        "        # Call forward method of every object in a chain\n",
        "        # Pass output of the previous object as a parameter\n",
        "        for layer in self.layers:\n",
        "            layer.forward(layer.prev.output, training)\n",
        "\n",
        "        # \"layer\" is now the last object from the list,\n",
        "        # return its output\n",
        "        return layer.output\n",
        "\n",
        "\n",
        "    # Performs backward pass\n",
        "    def backward(self, output, y):\n",
        "\n",
        "        # If softmax classifier\n",
        "        if self.softmax_classifier_output is not None:\n",
        "            # First call backward method\n",
        "            # on the combined activation/loss\n",
        "            # this will set dinputs property\n",
        "            self.softmax_classifier_output.backward(output, y)\n",
        "\n",
        "            # Since we'll not call backward method of the last layer\n",
        "            # which is Softmax activation\n",
        "            # as we used combined activation/loss\n",
        "            # object, let's set dinputs in this object\n",
        "            self.layers[-1].dinputs = \\\n",
        "                self.softmax_classifier_output.dinputs\n",
        "\n",
        "            # Call backward method going through\n",
        "            # all the objects but last\n",
        "            # in reversed order passing dinputs as a parameter\n",
        "            for layer in reversed(self.layers[:-1]):\n",
        "                layer.backward(layer.next.dinputs)\n",
        "\n",
        "            return\n",
        "\n",
        "        # First call backward method on the loss\n",
        "        # this will set dinputs property that the last\n",
        "        # layer will try to access shortly\n",
        "        self.loss.backward(output, y)\n",
        "\n",
        "        # Call backward method going through all the objects\n",
        "        # in reversed order passing dinputs as a parameter\n",
        "        for layer in reversed(self.layers):\n",
        "            layer.backward(layer.next.dinputs)\n",
        "\n",
        "# Predicts on the samples\n",
        "    def predict(self, X, *, batch_size=None):\n",
        "\n",
        "        # Default value if batch size is not being set\n",
        "        prediction_steps = 1\n",
        "\n",
        "        # Calculate number of steps\n",
        "        if batch_size is not None:\n",
        "            prediction_steps = len(X) // batch_size\n",
        "\n",
        "            # Dividing rounds down. If there are some remaining\n",
        "            # data but not a full batch, this won't include it\n",
        "            # Add `1` to include this not full batch\n",
        "            if prediction_steps * batch_size < len(X):\n",
        "                prediction_steps += 1\n",
        "\n",
        "        # Model outputs\n",
        "        output = []\n",
        "\n",
        "        # Iterate over steps\n",
        "        for step in range(prediction_steps):\n",
        "\n",
        "            # If batch size is not set -\n",
        "            # train using one step and full dataset\n",
        "            if batch_size is None:\n",
        "                batch_X = X\n",
        "\n",
        "            # Otherwise slice a batch\n",
        "            else:\n",
        "                batch_X = X[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "            # Perform the forward pass\n",
        "            batch_output = self.forward(batch_X, training=False)\n",
        "\n",
        "            # Append batch prediction to the list of predictions\n",
        "            output.append(batch_output)\n",
        "\n",
        "        # Stack and return results\n",
        "        return np.vstack(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating dataset and training our model:"
      ],
      "metadata": {
        "id": "ecPoIUmgKdDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import cv2\n",
        "\n",
        "# Create dataset\n",
        "(X, y), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "# Shuffle the training dataset\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X = X[keys]\n",
        "y = y[keys]\n",
        "\n",
        "# Scale and reshape samples\n",
        "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
        "             127.5) / 127.5\n",
        "\n",
        "# Instantiate the model\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# Add layers\n",
        "model.add(Layer_Dense(X.shape[1], 128))\n",
        "model.add(Activation_ReLU())\n",
        "model.add(Layer_Dense(128, 128))\n",
        "model.add(Activation_ReLU())\n",
        "model.add(Layer_Dense(128, 10))\n",
        "model.add(Activation_Softmax())\n",
        "\n",
        "# Set loss, optimizer and accuracy objects\n",
        "model.set(\n",
        "    loss=Loss_CategoricalCrossentropy(),\n",
        "    optimizer=Optimizer_Adam(decay=1e-3),\n",
        "    accuracy=Accuracy_Categorical()\n",
        ")\n",
        "\n",
        "# Finalize the model\n",
        "model.finalize()\n",
        "\n",
        "# Train the model\n",
        "model.train(X, y, validation_data=(X_test, y_test),\n",
        "            epochs=10, batch_size=128, print_every=100)"
      ],
      "metadata": {
        "id": "mjOtHXMKKNka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e35d2a-46e2-4608-9e48-0f2f00f2c885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "step: 0, acc: 0.086, loss: 2.303, lr: 0.001\n",
            "step: 100, acc: 0.641, loss: 0.807, lr: 0.0009090909090909091\n",
            "step: 200, acc: 0.781, loss: 0.563, lr: 0.0008333333333333334\n",
            "step: 300, acc: 0.773, loss: 0.503, lr: 0.0007692307692307692\n",
            "step: 400, acc: 0.820, loss: 0.397, lr: 0.0007142857142857143\n",
            "step: 468, acc: 0.906, loss: 0.347, lr: 0.000681198910081744\n",
            "training, acc: 0.758, loss: 0.653, lr: 0.000681198910081744\n",
            "validation, acc: 0.824, loss: 0.479\n",
            "epoch: 2\n",
            "step: 0, acc: 0.812, loss: 0.454, lr: 0.0006807351940095304\n",
            "step: 100, acc: 0.820, loss: 0.428, lr: 0.0006373486297004461\n",
            "step: 200, acc: 0.852, loss: 0.441, lr: 0.0005991611743559018\n",
            "step: 300, acc: 0.891, loss: 0.336, lr: 0.0005652911249293386\n",
            "step: 400, acc: 0.852, loss: 0.314, lr: 0.0005350454788657037\n",
            "step: 468, acc: 0.938, loss: 0.271, lr: 0.0005162622612287042\n",
            "training, acc: 0.844, loss: 0.428, lr: 0.0005162622612287042\n",
            "validation, acc: 0.845, loss: 0.428\n",
            "epoch: 3\n",
            "step: 0, acc: 0.859, loss: 0.390, lr: 0.0005159958720330237\n",
            "step: 100, acc: 0.859, loss: 0.377, lr: 0.0004906771344455348\n",
            "step: 200, acc: 0.844, loss: 0.394, lr: 0.0004677268475210477\n",
            "step: 300, acc: 0.891, loss: 0.271, lr: 0.00044682752457551384\n",
            "step: 400, acc: 0.859, loss: 0.286, lr: 0.00042771599657827206\n",
            "step: 468, acc: 0.938, loss: 0.243, lr: 0.0004156275976724854\n",
            "training, acc: 0.859, loss: 0.385, lr: 0.0004156275976724854\n",
            "validation, acc: 0.853, loss: 0.406\n",
            "epoch: 4\n",
            "step: 0, acc: 0.875, loss: 0.360, lr: 0.0004154549231408392\n",
            "step: 100, acc: 0.859, loss: 0.351, lr: 0.00039888312724371757\n",
            "step: 200, acc: 0.859, loss: 0.369, lr: 0.0003835826620636747\n",
            "step: 300, acc: 0.891, loss: 0.245, lr: 0.0003694126339120798\n",
            "step: 400, acc: 0.875, loss: 0.268, lr: 0.0003562522265764161\n",
            "step: 468, acc: 0.938, loss: 0.230, lr: 0.00034782608695652176\n",
            "training, acc: 0.868, loss: 0.360, lr: 0.00034782608695652176\n",
            "validation, acc: 0.857, loss: 0.393\n",
            "epoch: 5\n",
            "step: 0, acc: 0.875, loss: 0.341, lr: 0.0003477051460361613\n",
            "step: 100, acc: 0.867, loss: 0.329, lr: 0.00033602150537634406\n",
            "step: 200, acc: 0.859, loss: 0.354, lr: 0.00032509752925877764\n",
            "step: 300, acc: 0.914, loss: 0.235, lr: 0.00031486146095717883\n",
            "step: 400, acc: 0.898, loss: 0.256, lr: 0.00030525030525030525\n",
            "step: 468, acc: 0.938, loss: 0.217, lr: 0.0002990430622009569\n",
            "training, acc: 0.875, loss: 0.341, lr: 0.0002990430622009569\n",
            "validation, acc: 0.861, loss: 0.383\n",
            "epoch: 6\n",
            "step: 0, acc: 0.883, loss: 0.325, lr: 0.0002989536621823617\n",
            "step: 100, acc: 0.883, loss: 0.312, lr: 0.00029027576197387516\n",
            "step: 200, acc: 0.875, loss: 0.339, lr: 0.0002820874471086037\n",
            "step: 300, acc: 0.914, loss: 0.236, lr: 0.00027434842249657066\n",
            "step: 400, acc: 0.898, loss: 0.242, lr: 0.000267022696929239\n",
            "step: 468, acc: 0.927, loss: 0.204, lr: 0.00026226068712300026\n",
            "training, acc: 0.881, loss: 0.327, lr: 0.00026226068712300026\n",
            "validation, acc: 0.865, loss: 0.376\n",
            "epoch: 7\n",
            "step: 0, acc: 0.883, loss: 0.312, lr: 0.00026219192448872575\n",
            "step: 100, acc: 0.891, loss: 0.292, lr: 0.00025549310168625444\n",
            "step: 200, acc: 0.883, loss: 0.330, lr: 0.00024912805181863477\n",
            "step: 300, acc: 0.906, loss: 0.237, lr: 0.0002430724355858046\n",
            "step: 400, acc: 0.906, loss: 0.229, lr: 0.00023730422401518745\n",
            "step: 468, acc: 0.938, loss: 0.196, lr: 0.00023353573096683791\n",
            "training, acc: 0.885, loss: 0.315, lr: 0.00023353573096683791\n",
            "validation, acc: 0.866, loss: 0.369\n",
            "epoch: 8\n",
            "step: 0, acc: 0.891, loss: 0.302, lr: 0.00023348120476301658\n",
            "step: 100, acc: 0.906, loss: 0.272, lr: 0.00022815423226100847\n",
            "step: 200, acc: 0.883, loss: 0.323, lr: 0.0002230649118893598\n",
            "step: 300, acc: 0.914, loss: 0.236, lr: 0.00021819768710451667\n",
            "step: 400, acc: 0.906, loss: 0.218, lr: 0.00021353833013025838\n",
            "step: 468, acc: 0.938, loss: 0.190, lr: 0.00021048200378867611\n",
            "training, acc: 0.889, loss: 0.306, lr: 0.00021048200378867611\n",
            "validation, acc: 0.869, loss: 0.364\n",
            "epoch: 9\n",
            "step: 0, acc: 0.898, loss: 0.292, lr: 0.0002104377104377104\n",
            "step: 100, acc: 0.906, loss: 0.256, lr: 0.0002061005770816158\n",
            "step: 200, acc: 0.883, loss: 0.318, lr: 0.00020193861066235866\n",
            "step: 300, acc: 0.906, loss: 0.235, lr: 0.0001979414093428345\n",
            "step: 400, acc: 0.914, loss: 0.211, lr: 0.0001940993788819876\n",
            "step: 468, acc: 0.948, loss: 0.184, lr: 0.00019157088122605365\n",
            "training, acc: 0.892, loss: 0.298, lr: 0.00019157088122605365\n",
            "validation, acc: 0.871, loss: 0.359\n",
            "epoch: 10\n",
            "step: 0, acc: 0.914, loss: 0.283, lr: 0.0001915341888527102\n",
            "step: 100, acc: 0.922, loss: 0.243, lr: 0.00018793459875963167\n",
            "step: 200, acc: 0.883, loss: 0.312, lr: 0.00018446781036709093\n",
            "step: 300, acc: 0.906, loss: 0.230, lr: 0.00018112660749864155\n",
            "step: 400, acc: 0.914, loss: 0.204, lr: 0.00017790428749332856\n",
            "step: 468, acc: 0.958, loss: 0.180, lr: 0.00017577781683951485\n",
            "training, acc: 0.894, loss: 0.291, lr: 0.00017577781683951485\n",
            "validation, acc: 0.873, loss: 0.354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our model for predictions - image classification:"
      ],
      "metadata": {
        "id": "AMAdSmViK8pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label index to label name relation\n",
        "fashion_mnist_labels = {\n",
        "    0: 'T-shirt/top',\n",
        "    1: 'Trouser',\n",
        "    2: 'Pullover',\n",
        "    3: 'Dress',\n",
        "    4: 'Coat',\n",
        "    5: 'Sandal',\n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker',\n",
        "    8: 'Bag',\n",
        "    9: 'Ankle boot'\n",
        "}\n",
        "\n",
        "\n",
        "image = 't_shirt.jpg'\n",
        "\n",
        "# Read an image\n",
        "image_data = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Resize to the same size as Fashion MNIST images\n",
        "image_data = cv2.resize(image_data, (28, 28))\n",
        "\n",
        "#np.set_printoptions(linewidth=200)\n",
        "#print(image_data)\n",
        "\n",
        "# Invert image colors\n",
        "image_data = 255 - image_data\n",
        "\n",
        "# Reshape and scale pixel data\n",
        "image_data = (image_data.reshape(1, -1).astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "\n",
        "# Predict on the image\n",
        "confidences = model.predict(image_data)\n",
        "\n",
        "# Get prediction instead of confidence levels\n",
        "predictions = model.output_layer_activation.predictions(confidences)\n",
        "\n",
        "# Get label name from label index\n",
        "prediction = fashion_mnist_labels[predictions[0]]\n",
        "\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfvH23vKEI3v",
        "outputId": "e011559f-9fee-4b40-b954-f04a65ee0624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-shirt/top\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equivalent code but using PyTorch:"
      ],
      "metadata": {
        "id": "n0HfIGi3RgL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# Define the neural network model\n",
        "class PyTorchModel(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_size1=128, hidden_size2=128, output_size=10):\n",
        "        super(PyTorchModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "        # Note: CrossEntropyLoss includes Softmax, so we don't add it here\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = PyTorchModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        print(f'epoch: {epoch+1}')\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Print every 100 batches\n",
        "            if i % 100 == 0 or i == len(train_loader) - 1:\n",
        "                accuracy = 100 * correct / total\n",
        "                print(f'step: {i}, '\n",
        "                      f'acc: {accuracy/100:.3f}, '\n",
        "                      f'loss: {running_loss/(i+1):.3f}')\n",
        "\n",
        "        # Calculate training accuracy and loss for the epoch\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "        print(f'training, '\n",
        "              f'acc: {epoch_accuracy/100:.3f}, '\n",
        "              f'loss: {epoch_loss:.3f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        val_loss = val_running_loss / len(test_loader)\n",
        "\n",
        "        print(f'validation, '\n",
        "              f'acc: {val_accuracy/100:.3f}, '\n",
        "              f'loss: {val_loss:.3f}')\n",
        "        print()\n",
        "\n",
        "\n",
        "def predict(model, X):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            if isinstance(X, np.ndarray):\n",
        "                X = torch.FloatTensor(X)\n",
        "            outputs = model(X)\n",
        "            return outputs.numpy()\n",
        "\n",
        "# Load Fashion MNIST dataset using PyTorch\n",
        "def load_fashion_mnist():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = FashionMNIST(root='./data', train=True,\n",
        "                                download=True, transform=transform)\n",
        "    test_dataset = FashionMNIST(root='./data', train=False,\n",
        "                               download=True, transform=transform)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# Load data\n",
        "train_dataset, test_dataset = load_fashion_mnist()\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPgBN1YdRcxk",
        "outputId": "0586b986-0909-4d3d-9ddc-9d2418349d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.4MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 386kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 6.17MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 14.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "step: 0, acc: 0.039, loss: 2.313\n",
            "step: 100, acc: 0.706, loss: 0.832\n",
            "step: 200, acc: 0.763, loss: 0.667\n",
            "step: 300, acc: 0.785, loss: 0.601\n",
            "step: 400, acc: 0.800, loss: 0.559\n",
            "step: 468, acc: 0.804, loss: 0.546\n",
            "training, acc: 0.804, loss: 0.546\n",
            "validation, acc: 0.831, loss: 0.451\n",
            "\n",
            "epoch: 2\n",
            "step: 0, acc: 0.906, loss: 0.298\n",
            "step: 100, acc: 0.850, loss: 0.407\n",
            "step: 200, acc: 0.852, loss: 0.401\n",
            "step: 300, acc: 0.854, loss: 0.399\n",
            "step: 400, acc: 0.855, loss: 0.397\n",
            "step: 468, acc: 0.855, loss: 0.395\n",
            "training, acc: 0.855, loss: 0.395\n",
            "validation, acc: 0.853, loss: 0.400\n",
            "\n",
            "epoch: 3\n",
            "step: 0, acc: 0.859, loss: 0.361\n",
            "step: 100, acc: 0.865, loss: 0.365\n",
            "step: 200, acc: 0.868, loss: 0.364\n",
            "step: 300, acc: 0.865, loss: 0.370\n",
            "step: 400, acc: 0.867, loss: 0.366\n",
            "step: 468, acc: 0.868, loss: 0.364\n",
            "training, acc: 0.868, loss: 0.364\n",
            "validation, acc: 0.853, loss: 0.403\n",
            "\n",
            "epoch: 4\n",
            "step: 0, acc: 0.812, loss: 0.507\n",
            "step: 100, acc: 0.877, loss: 0.338\n",
            "step: 200, acc: 0.874, loss: 0.345\n",
            "step: 300, acc: 0.874, loss: 0.346\n",
            "step: 400, acc: 0.874, loss: 0.344\n",
            "step: 468, acc: 0.874, loss: 0.345\n",
            "training, acc: 0.874, loss: 0.345\n",
            "validation, acc: 0.861, loss: 0.371\n",
            "\n",
            "epoch: 5\n",
            "step: 0, acc: 0.875, loss: 0.342\n",
            "step: 100, acc: 0.884, loss: 0.324\n",
            "step: 200, acc: 0.882, loss: 0.326\n",
            "step: 300, acc: 0.882, loss: 0.326\n",
            "step: 400, acc: 0.881, loss: 0.327\n",
            "step: 468, acc: 0.880, loss: 0.329\n",
            "training, acc: 0.880, loss: 0.329\n",
            "validation, acc: 0.862, loss: 0.380\n",
            "\n",
            "epoch: 6\n",
            "step: 0, acc: 0.883, loss: 0.392\n",
            "step: 100, acc: 0.882, loss: 0.316\n",
            "step: 200, acc: 0.883, loss: 0.317\n",
            "step: 300, acc: 0.884, loss: 0.317\n",
            "step: 400, acc: 0.883, loss: 0.319\n",
            "step: 468, acc: 0.882, loss: 0.323\n",
            "training, acc: 0.882, loss: 0.323\n",
            "validation, acc: 0.868, loss: 0.366\n",
            "\n",
            "epoch: 7\n",
            "step: 0, acc: 0.914, loss: 0.342\n",
            "step: 100, acc: 0.885, loss: 0.318\n",
            "step: 200, acc: 0.885, loss: 0.313\n",
            "step: 300, acc: 0.887, loss: 0.309\n",
            "step: 400, acc: 0.886, loss: 0.312\n",
            "step: 468, acc: 0.886, loss: 0.312\n",
            "training, acc: 0.886, loss: 0.312\n",
            "validation, acc: 0.871, loss: 0.356\n",
            "\n",
            "epoch: 8\n",
            "step: 0, acc: 0.875, loss: 0.422\n",
            "step: 100, acc: 0.885, loss: 0.310\n",
            "step: 200, acc: 0.887, loss: 0.304\n",
            "step: 300, acc: 0.886, loss: 0.306\n",
            "step: 400, acc: 0.887, loss: 0.304\n",
            "step: 468, acc: 0.888, loss: 0.304\n",
            "training, acc: 0.888, loss: 0.304\n",
            "validation, acc: 0.866, loss: 0.368\n",
            "\n",
            "epoch: 9\n",
            "step: 0, acc: 0.922, loss: 0.262\n",
            "step: 100, acc: 0.894, loss: 0.295\n",
            "step: 200, acc: 0.891, loss: 0.295\n",
            "step: 300, acc: 0.891, loss: 0.298\n",
            "step: 400, acc: 0.890, loss: 0.300\n",
            "step: 468, acc: 0.890, loss: 0.299\n",
            "training, acc: 0.890, loss: 0.299\n",
            "validation, acc: 0.872, loss: 0.354\n",
            "\n",
            "epoch: 10\n",
            "step: 0, acc: 0.914, loss: 0.286\n",
            "step: 100, acc: 0.893, loss: 0.288\n",
            "step: 200, acc: 0.895, loss: 0.286\n",
            "step: 300, acc: 0.895, loss: 0.288\n",
            "step: 400, acc: 0.893, loss: 0.290\n",
            "step: 468, acc: 0.892, loss: 0.293\n",
            "training, acc: 0.892, loss: 0.293\n",
            "validation, acc: 0.877, loss: 0.341\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PyTorch Model for predidctions:"
      ],
      "metadata": {
        "id": "cT0Hc5FfWlCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the image\n",
        "confidences = predict(model, image_data)\n",
        "\n",
        "# Get prediction\n",
        "prediction = np.argmax(confidences, axis=1)\n",
        "\n",
        "# Get label name from label index\n",
        "prediction_label = fashion_mnist_labels[prediction[0]]\n",
        "\n",
        "print(f\"Prediction: {prediction_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12wrAm8pWaAB",
        "outputId": "fcc36b95-e52f-40f4-c61f-c796a99a34fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: T-shirt/top\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equivalent code but using scikit-learn:"
      ],
      "metadata": {
        "id": "ddpwpWziRp0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.datasets import fashion_mnist # Easiest way to load the data\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "\n",
        "# Load data using Keras helper\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Reshape and scale the data to [-1, 1], same as the original code\n",
        "X_train = (X_train.reshape(X_train.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "# --- 2. Model Definition and Training ---\n",
        "\n",
        "# MLPClassifier encapsulates the model, optimizer, and training loop\n",
        "# We configure it to match the original setup\n",
        "model = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 128),  # Two hidden layers with 128 neurons each\n",
        "    activation='relu',               # ReLU activation function\n",
        "    solver='adam',                   # Adam optimizer\n",
        "    batch_size=128,\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=10,                     # Number of epochs\n",
        "    verbose=True,                    # Print progress during training\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Starting training with Scikit-learn...\")\n",
        "# The .fit() method handles the entire training process\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M4gUL2-RqtU",
        "outputId": "e46b30b6-5062-44e9-8317-1b253ba6d20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with Scikit-learn...\n",
            "Iteration 1, loss = 0.50288713\n",
            "Iteration 2, loss = 0.36840914\n",
            "Iteration 3, loss = 0.33148999\n",
            "Iteration 4, loss = 0.30896976\n",
            "Iteration 5, loss = 0.28979853\n",
            "Iteration 6, loss = 0.27450975\n",
            "Iteration 7, loss = 0.26111436\n",
            "Iteration 8, loss = 0.25079489\n",
            "Iteration 9, loss = 0.24163692\n",
            "Iteration 10, loss = 0.22985119\n",
            "\n",
            "Test Accuracy: 0.8778\n",
            "\n",
            "Confidences: [[9.8920548e-01 1.5849753e-06 1.7031074e-03 1.2575094e-04 4.0785231e-05 4.7893705e-06 7.9472084e-03 8.0662737e-08 9.7124022e-04 1.3379148e-08]]\n",
            "Prediction: T-shirt/top\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Scikit-learn Model for predidctions:"
      ],
      "metadata": {
        "id": "Dt8MoAnZY9iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get probability scores (confidences)\n",
        "confidences = model.predict_proba(image_data)\n",
        "\n",
        "# Get the final prediction\n",
        "prediction_index = model.predict(image_data)[0]\n",
        "prediction_label = fashion_mnist_labels[prediction_index]\n",
        "\n",
        "print(f\"Prediction: {prediction_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnQh06-jY9Cl",
        "outputId": "7eb624f6-e8ae-4746-cf5d-c41e9c383140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Trouser\n"
          ]
        }
      ]
    }
  ]
}